{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Processing Script Explanation\n",
    "\n",
    "This notebook explains how the `process_json_wash.py` script works. The script processes OCR text files and their associated tables into an optimized JSON format for the NYC Plumbing Code.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The script performs these main tasks:\n",
    "1. Reads OCR text files\n",
    "2. Extracts tables and analytics data\n",
    "3. Organizes content by chapters and sections\n",
    "4. Creates an optimized JSON structure\n",
    "\n",
    "Let's go through each part in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Script Setup and Configuration\n",
    "\n",
    "First, we import required modules and set up paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = Path(\"/Users/aaronjpeters/PlumbingCodeAi/BuildingCodeai\")\n",
    "MEDIA_ROOT = BASE_DIR / \"media\"\n",
    "PLUMBING_CODE_DIR = MEDIA_ROOT / \"plumbing_code\"\n",
    "\n",
    "# Directory structure\n",
    "PLUMBING_CODE_DIRS = {\n",
    "    \"ocr\": PLUMBING_CODE_DIR / \"OCR\",        # OCR text files\n",
    "    \"json\": PLUMBING_CODE_DIR / \"json\",      # Initial JSON files\n",
    "    \"json_processed\": PLUMBING_CODE_DIR / \"json_processed\",  # Final JSON files\n",
    "    \"tables\": PLUMBING_CODE_DIR / \"tables\",  # Table data files\n",
    "    \"analytics\": PLUMBING_CODE_DIR / \"analytics\",  # Analytics images\n",
    "    \"optimized\": PLUMBING_CODE_DIR / \"optimized\"   # Optimized images\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading Table Data\n",
    "\n",
    "The `read_table_data` function reads and parses table data from text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def read_table_data(table_path: str) -> Optional[Dict]:\n",
    "    \"\"\"Read and parse table data from file.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(table_path):\n",
    "            return None\n",
    "            \n",
    "        with open(table_path, 'r', encoding='utf-8') as f:\n",
    "            table_content = f.read()\n",
    "            \n",
    "        return {\n",
    "            'table_content': table_content,\n",
    "            'table_path': table_path,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading table data from {table_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processing Individual Files\n",
    "\n",
    "The `process_file` function handles a single OCR text file and its associated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_file(text_path: str) -> Dict:\n",
    "    \"\"\"Process a single text file and its associated table data.\"\"\"\n",
    "    # Read OCR text\n",
    "    with open(text_path, 'r', encoding='utf-8') as f:\n",
    "        text_content = f.read()\n",
    "        \n",
    "    # Extract page number from filename\n",
    "    filename = Path(text_path).stem\n",
    "    pg_num = int(''.join(filter(str.isdigit, filename.split('_')[-1])))\n",
    "    \n",
    "    # Check for associated files\n",
    "    table_file = PLUMBING_CODE_DIRS[\"tables\"] / f\"{filename}.txt\"\n",
    "    analytics_file = PLUMBING_CODE_DIRS[\"analytics\"] / f\"{filename}.png\"\n",
    "    \n",
    "    # Create file entry\n",
    "    file_entry = {\n",
    "        \"i\": pg_num,              # Page index\n",
    "        \"p\": str(text_path),      # Path to text file\n",
    "        \"o\": str(PLUMBING_CODE_DIRS[\"optimized\"] / f\"{filename}.jpg\"),  # Optimized image\n",
    "        \"pg\": pg_num,             # Page number\n",
    "        \"t\": text_content         # Text content\n",
    "    }\n",
    "    \n",
    "    # Add table data if exists\n",
    "    if table_file.exists():\n",
    "        table_data = read_table_data(str(table_file))\n",
    "        file_entry[\"tb\"] = table_data[\"table_content\"]\n",
    "        file_entry[\"tb_data\"] = str(table_file)\n",
    "        \n",
    "    # Add analytics image if exists\n",
    "    if analytics_file.exists():\n",
    "        file_entry[\"tb_img\"] = str(analytics_file)\n",
    "        \n",
    "    return file_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Processing the Directory\n",
    "\n",
    "The `process_directory` function processes all files in the OCR directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_directory(base_dir: str) -> Dict[str, Dict]:\n",
    "    \"\"\"Process all text files in the OCR directory.\"\"\"\n",
    "    processed_data = {}\n",
    "    \n",
    "    # Process each text file\n",
    "    for file_path in PLUMBING_CODE_DIRS[\"ocr\"].glob(\"*.txt\"):\n",
    "        # Extract chapter number (e.g., NYCP1CH -> 1)\n",
    "        chapter_match = re.search(r'NYCP(\\d+)CH', file_path.stem, re.IGNORECASE)\n",
    "        if not chapter_match:\n",
    "            continue\n",
    "            \n",
    "        chapter_num = chapter_match.group(1)\n",
    "        chapter_key = f\"NYCP{chapter_num}CH_\"\n",
    "        \n",
    "        # Initialize chapter data\n",
    "        if chapter_key not in processed_data:\n",
    "            processed_data[chapter_key] = {\n",
    "                \"m\": {                # Metadata\n",
    "                    \"c\": chapter_num,  # Chapter number\n",
    "                    \"t\": \"NYCPC\",     # Title\n",
    "                    \"ct\": \"\"          # Chapter title\n",
    "                },\n",
    "                \"f\": [],             # Files\n",
    "                \"r\": [],             # Raw text\n",
    "                \"s\": [],             # Sections\n",
    "                \"tb\": []             # Tables\n",
    "            }\n",
    "        \n",
    "        # Process the file\n",
    "        file_entry = process_file(str(file_path))\n",
    "        \n",
    "        # Add file entry\n",
    "        processed_data[chapter_key][\"f\"].append(file_entry)\n",
    "        \n",
    "        # Add raw text\n",
    "        if \"t\" in file_entry:\n",
    "            processed_data[chapter_key][\"r\"].append({\n",
    "                \"i\": file_entry[\"i\"],\n",
    "                \"t\": file_entry[\"t\"]\n",
    "            })\n",
    "        \n",
    "        # Add table if exists\n",
    "        if \"tb\" in file_entry:\n",
    "            processed_data[chapter_key][\"tb\"].append({\n",
    "                \"i\": file_entry[\"i\"],\n",
    "                \"t\": file_entry[\"tb\"],\n",
    "                \"f\": file_entry[\"i\"],\n",
    "                \"d\": file_entry.get(\"tb_data\"),\n",
    "                \"img\": file_entry.get(\"tb_img\")\n",
    "            })\n",
    "        \n",
    "        # Process sections\n",
    "        if \"t\" in file_entry:\n",
    "            sections = []\n",
    "            current_section = None\n",
    "            \n",
    "            for line in file_entry[\"t\"].split(\"\\n\"):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                # Look for section headers (e.g., \"308.5 Interval of support\")\n",
    "                section_match = re.match(r\"^(\\d+\\.\\d+(?:\\.\\d+)?)\\s+(.+)$\", line)\n",
    "                if section_match:\n",
    "                    if current_section:\n",
    "                        sections.append(current_section)\n",
    "                    section_id = section_match.group(1)\n",
    "                    current_section = {\n",
    "                        \"i\": section_id,    # Section ID\n",
    "                        \"t\": line,          # Section title\n",
    "                        \"c\": \"\",           # Section content\n",
    "                        \"f\": file_entry[\"i\"]  # Source file\n",
    "                    }\n",
    "                elif current_section:\n",
    "                    current_section[\"c\"] += line + \"\\n\"\n",
    "            \n",
    "            if current_section:\n",
    "                sections.append(current_section)\n",
    "            \n",
    "            # Add sections\n",
    "            processed_data[chapter_key][\"s\"].extend(sections)\n",
    "    \n",
    "    # Sort sections numerically\n",
    "    for chapter_data in processed_data.values():\n",
    "        chapter_data[\"s\"].sort(key=lambda s: tuple(float(p) for p in s[\"i\"].split(\".\")))\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving JSON Files\n",
    "\n",
    "The `save_json` function saves the processed data to JSON files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_json(data: Dict[str, Dict], output_dir: str) -> None:\n",
    "    \"\"\"Save processed data to JSON files.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for filename, chapter_data in data.items():\n",
    "        output_file = os.path.join(output_dir, f\"{filename}.json\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chapter_data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output JSON Structure\n",
    "\n",
    "The final JSON structure for each chapter looks like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"m\": {                    // Metadata\n",
    "    \"c\": \"3\",              // Chapter number\n",
    "    \"t\": \"NYCPC\",          // Title\n",
    "    \"ct\": \"ADMINISTRATION\" // Chapter title\n",
    "  },\n",
    "  \"f\": [                    // Files\n",
    "    {\n",
    "      \"i\": 2,              // Page index\n",
    "      \"p\": \"path/to/text.txt\",  // Text file path\n",
    "      \"o\": \"path/to/image.jpg\", // Optimized image path\n",
    "      \"pg\": 2,             // Page number\n",
    "      \"t\": \"text content\", // Text content\n",
    "      \"tb\": \"table content\", // Table content (if exists)\n",
    "      \"tb_data\": \"path/to/table.txt\", // Table file path\n",
    "      \"tb_img\": \"path/to/analytics.png\" // Analytics image path\n",
    "    }\n",
    "  ],\n",
    "  \"r\": [                    // Raw text entries\n",
    "    {\n",
    "      \"i\": 2,              // Page index\n",
    "      \"t\": \"text content\"  // Text content\n",
    "    }\n",
    "  ],\n",
    "  \"s\": [                    // Sections\n",
    "    {\n",
    "      \"i\": \"308.5\",        // Section ID\n",
    "      \"t\": \"308.5 Interval of support\", // Section title\n",
    "      \"c\": \"section content\", // Section content\n",
    "      \"f\": 2               // Source file index\n",
    "    }\n",
    "  ],\n",
    "  \"tb\": [                   // Tables\n",
    "    {\n",
    "      \"i\": 2,              // Page index\n",
    "      \"t\": \"table content\", // Table content\n",
    "      \"f\": 2,              // Source file index\n",
    "      \"d\": \"path/to/table.txt\", // Table file path\n",
    "      \"img\": \"path/to/analytics.png\" // Analytics image path\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "This structure organizes the plumbing code data into a format that's easy to query and use in applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
