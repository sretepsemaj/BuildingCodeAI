{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processor Deep Dive\n",
    "\n",
    "This notebook provides a comprehensive explanation of the Image Processor module, which handles image processing and OCR (Optical Character Recognition) tasks in our plumbing code project.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "# Add project root to Python path\n",
    "import sys\n",
    "project_root = '/Users/aaronjpeters/PlumbingCodeAi/BuildingCodeai'\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import our processor\n",
    "from main.utils.image_processor import ImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Image Processing Pipeline\n",
    "\n",
    "The Image Processor handles various image processing tasks including:\n",
    "1. Image preprocessing\n",
    "2. OCR text extraction\n",
    "3. Image enhancement\n",
    "4. Metadata extraction\n",
    "\n",
    "Let's explore each component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the processor\n",
    "image_processor = ImageProcessor()\n",
    "\n",
    "def demonstrate_preprocessing(image_path: str):\n",
    "    \"\"\"Demonstrate image preprocessing steps.\"\"\"\n",
    "    # Read image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply thresholding\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Denoise\n",
    "    denoised = cv2.fastNlMeansDenoising(binary)\n",
    "    \n",
    "    return {\n",
    "        'original': image,\n",
    "        'grayscale': gray,\n",
    "        'binary': binary,\n",
    "        'denoised': denoised\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# results = demonstrate_preprocessing('path/to/image.png')\n",
    "# for name, img in results.items():\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.imshow(img, cmap='gray')\n",
    "#     plt.title(name)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OCR Text Extraction\n",
    "\n",
    "The processor uses Tesseract OCR to extract text from images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def demonstrate_ocr(image_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Demonstrate OCR text extraction.\"\"\"\n",
    "    try:\n",
    "        # Process image\n",
    "        preprocessed = demonstrate_preprocessing(image_path)\n",
    "        if preprocessed is None:\n",
    "            return {'error': 'Failed to load image'}\n",
    "        \n",
    "        # Extract text from different versions\n",
    "        results = {}\n",
    "        for name, img in preprocessed.items():\n",
    "            # Convert to PIL Image\n",
    "            pil_img = Image.fromarray(img)\n",
    "            \n",
    "            # Extract text\n",
    "            text = pytesseract.image_to_string(pil_img)\n",
    "            results[f'{name}_text'] = text\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Example usage:\n",
    "# ocr_results = demonstrate_ocr('path/to/image.png')\n",
    "# for version, text in ocr_results.items():\n",
    "#     print(f\"\\n{version}:\\n{text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Base64 Image Encoding\n",
    "\n",
    "The Image Processor converts images to base64 format for efficient storage and transmission. This is particularly useful for:\n",
    "- Embedding images in JSON\n",
    "- Storing images in databases\n",
    "- Transmitting images over HTTP\n",
    "\n",
    "Let's explore the base64 encoding process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def demonstrate_base64_conversion(image_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Demonstrate base64 encoding of images.\"\"\"\n",
    "    try:\n",
    "        # Read image\n",
    "        with open(image_path, 'rb') as img_file:\n",
    "            img_data = img_file.read()\n",
    "        \n",
    "        # Convert to base64\n",
    "        base64_str = base64.b64encode(img_data).decode('utf-8')\n",
    "        \n",
    "        # Create a sample metadata structure\n",
    "        metadata = {\n",
    "            'file_name': os.path.basename(image_path),\n",
    "            'file_size': len(img_data),\n",
    "            'encoding': 'base64',\n",
    "            'mime_type': 'image/png',  # Adjust based on actual image type\n",
    "            'base64_data': base64_str\n",
    "        }\n",
    "        \n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Example usage:\n",
    "# metadata = demonstrate_base64_conversion('path/to/image.png')\n",
    "# print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Back from Base64\n",
    "\n",
    "We can also convert base64 strings back to images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def base64_to_image(base64_str: str) -> np.ndarray:\n",
    "    \"\"\"Convert base64 string back to image.\"\"\"\n",
    "    try:\n",
    "        # Decode base64\n",
    "        img_data = base64.b64decode(base64_str)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        nparr = np.frombuffer(img_data, np.uint8)\n",
    "        \n",
    "        # Decode image\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        return img\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting base64 to image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "# metadata = demonstrate_base64_conversion('path/to/image.png')\n",
    "# base64_str = metadata['base64_data']\n",
    "# recovered_image = base64_to_image(base64_str)\n",
    "# if recovered_image is not None:\n",
    "#     plt.imshow(cv2.cvtColor(recovered_image, cv2.COLOR_BGR2RGB))\n",
    "#     plt.title('Recovered from base64')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Base64 Conversion\n",
    "\n",
    "Tips for efficient base64 handling:\n",
    "1. **Size Optimization**\n",
    "   - Compress images before encoding\n",
    "   - Consider image quality vs size tradeoffs\n",
    "   - Use appropriate image formats (PNG for text, JPEG for photos)\n",
    "\n",
    "2. **Memory Management**\n",
    "   - Process large images in chunks\n",
    "   - Clean up temporary files\n",
    "   - Use context managers for file handling\n",
    "\n",
    "3. **Error Handling**\n",
    "   - Validate base64 strings\n",
    "   - Check for corruption\n",
    "   - Handle decoding errors gracefully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Enhancement Techniques\n",
    "\n",
    "Various enhancement techniques are used to improve OCR accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def demonstrate_enhancement(image: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Demonstrate various image enhancement techniques.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Contrast enhancement\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced = cv2.merge((cl,a,b))\n",
    "    enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)\n",
    "    results['contrast_enhanced'] = enhanced\n",
    "    \n",
    "    # Sharpening\n",
    "    kernel = np.array([[0,-1,0], [-1,5,-1], [0,-1,0]])\n",
    "    sharpened = cv2.filter2D(image, -1, kernel)\n",
    "    results['sharpened'] = sharpened\n",
    "    \n",
    "    # Noise reduction\n",
    "    denoised = cv2.fastNlMeansDenoisingColored(image)\n",
    "    results['denoised'] = denoised\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# image = cv2.imread('path/to/image.png')\n",
    "# enhanced_results = demonstrate_enhancement(image)\n",
    "# for name, img in enhanced_results.items():\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "#     plt.title(name)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Processing Example\n",
    "\n",
    "Let's put everything together with a complete example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_complete_example(image_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process an image through the complete pipeline.\"\"\"\n",
    "    try:\n",
    "        # Initialize processor\n",
    "        processor = ImageProcessor()\n",
    "        \n",
    "        # Process the image\n",
    "        result = processor.process_file(image_path)\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'result': result\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "# result = process_complete_example('path/to/image.png')\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices and Tips\n",
    "\n",
    "1. **Image Preparation**\n",
    "   - Use high-resolution images\n",
    "   - Ensure good lighting and contrast\n",
    "   - Remove noise and artifacts\n",
    "\n",
    "2. **OCR Optimization**\n",
    "   - Preprocess images for better results\n",
    "   - Use appropriate thresholding\n",
    "   - Consider image orientation\n",
    "\n",
    "3. **Performance**\n",
    "   - Cache processed results\n",
    "   - Use batch processing for multiple images\n",
    "   - Optimize memory usage\n",
    "\n",
    "4. **Error Handling**\n",
    "   - Validate input images\n",
    "   - Handle OCR failures gracefully\n",
    "   - Provide meaningful error messages\n",
    "\n",
    "5. **Testing**\n",
    "   - Test with various image types\n",
    "   - Verify OCR accuracy\n",
    "   - Benchmark performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
